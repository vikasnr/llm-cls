{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>complexity</th>\n",
       "      <th>data_type</th>\n",
       "      <th>module</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>readability</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>num_named_entities</th>\n",
       "      <th>contains_table</th>\n",
       "      <th>contains_list</th>\n",
       "      <th>redundancy_score</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>contains_numbers</th>\n",
       "      <th>contains_chain_of_thought</th>\n",
       "      <th>contains_output_constraints</th>\n",
       "      <th>is_multi_turn</th>\n",
       "      <th>LLM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>Simple</td>\n",
       "      <td>['Text', 'Numerical Data']</td>\n",
       "      <td>document_extraction</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>Simple</td>\n",
       "      <td>['Tables']</td>\n",
       "      <td>document_extraction</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PID  prompt_length complexity                   data_type  \\\n",
       "0    0             60     Simple  ['Text', 'Numerical Data']   \n",
       "1    1             55     Simple                  ['Tables']   \n",
       "\n",
       "                module  num_tokens  num_sentences  lexical_diversity  \\\n",
       "0  document_extraction          11              1           0.909091   \n",
       "1  document_extraction          10              1           1.000000   \n",
       "\n",
       "   readability  noun_ratio  ...  num_named_entities  contains_table  \\\n",
       "0          8.4    0.272727  ...                   1           False   \n",
       "1          9.2    0.200000  ...                   0           False   \n",
       "\n",
       "   contains_list  redundancy_score  compression_ratio  contains_numbers  \\\n",
       "0          False                 1           5.454545             False   \n",
       "1          False                 0           5.500000             False   \n",
       "\n",
       "   contains_chain_of_thought  contains_output_constraints  is_multi_turn  LLM  \n",
       "0                      False                        False          False  GPT  \n",
       "1                      False                        False          False  GPT  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the dfset\n",
    "df = pd.read_csv(\"files/input_final.csv\")  # Replace with actual filename\n",
    "df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PID  prompt_length  complexity  data_type                              module                        num_tokens  num_sentences  lexical_diversity  readability  noun_ratio  verb_ratio  adjective_ratio  num_named_entities  contains_table  contains_list  redundancy_score  compression_ratio  contains_numbers  contains_chain_of_thought  contains_output_constraints  is_multi_turn  LLM   \n",
       "506  91             Complex     Text                                   multi_document_summarization  17          1              1.000000           10.3         0.176471    0.117647    0.176471         1                   False           False          1                 5.352941           True              False                      False                        False          GPT       1\n",
       "0    60             Simple      ['Text', 'Numerical Data']             document_extraction           11          1              0.909091           8.4          0.272727    0.181818    0.000000         1                   False           False          1                 5.454545           False             False                      False                        False          GPT       1\n",
       "1    55             Simple      ['Tables']                             document_extraction           10          1              1.000000           9.2          0.200000    0.200000    0.000000         0                   False           False          0                 5.500000           False             False                      False                        False          GPT       1\n",
       "2    95             Simple      ['Text']                               document_extraction           17          1              0.941176           10.3         0.235294    0.176471    0.000000         0                   False           False          1                 5.588235           False             False                      False                        False          GPT       1\n",
       "3    97             Simple      ['Dates & Timestamps']                 document_extraction           19          1              0.842105           9.5          0.315789    0.105263    0.052632         0                   False           False          2                 5.105263           False             False                      False                        False          GPT       1\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                   ..\n",
       "30   75             Simple      ['Numerical Data']                     document_extraction           13          1              1.000000           10.3         0.230769    0.153846    0.076923         0                   False           False          1                 5.769231           False             False                      False                        False          llma      1\n",
       "31   83             Simple      ['Text', 'Numerical Data']             document_extraction           14          1              0.928571           10.7         0.357143    0.142857    0.000000         0                   False           False          1                 5.928571           False             False                      False                        False          GPT       1\n",
       "32   77             Simple      ['Text', 'Numerical Data']             document_extraction           15          1              0.866667           6.4          0.333333    0.066667    0.000000         0                   False           False          1                 5.133333           False             False                      False                        False          GPT       1\n",
       "33   77             Simple      ['Dates & Timestamps']                 document_extraction           15          1              0.866667           6.4          0.266667    0.133333    0.000000         1                   False           False          1                 5.133333           False             False                      False                        False          GPT       1\n",
       "34   69             Simple      ['Graphs & Charts', 'Numerical Data']  document_extraction           14          1              0.857143           6.0          0.357143    0.071429    0.000000         0                   False           False          1                 4.928571           False             False                      False                        False          Claude    1\n",
       "Name: count, Length: 507, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop PIP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>complexity</th>\n",
       "      <th>module</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>readability</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "      <th>adjective_ratio</th>\n",
       "      <th>num_named_entities</th>\n",
       "      <th>contains_table</th>\n",
       "      <th>contains_list</th>\n",
       "      <th>redundancy_score</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>contains_numbers</th>\n",
       "      <th>contains_chain_of_thought</th>\n",
       "      <th>contains_output_constraints</th>\n",
       "      <th>is_multi_turn</th>\n",
       "      <th>LLM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>Simple</td>\n",
       "      <td>document_extraction</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>Simple</td>\n",
       "      <td>document_extraction</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95</td>\n",
       "      <td>Simple</td>\n",
       "      <td>document_extraction</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5.588235</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97</td>\n",
       "      <td>Simple</td>\n",
       "      <td>document_extraction</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>5.105263</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>Simple</td>\n",
       "      <td>document_extraction</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5.461538</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_length complexity               module  num_tokens  num_sentences  \\\n",
       "0             60     Simple  document_extraction          11              1   \n",
       "1             55     Simple  document_extraction          10              1   \n",
       "2             95     Simple  document_extraction          17              1   \n",
       "3             97     Simple  document_extraction          19              1   \n",
       "4             71     Simple  document_extraction          13              1   \n",
       "\n",
       "   lexical_diversity  readability  noun_ratio  verb_ratio  adjective_ratio  \\\n",
       "0           0.909091          8.4    0.272727    0.181818         0.000000   \n",
       "1           1.000000          9.2    0.200000    0.200000         0.000000   \n",
       "2           0.941176         10.3    0.235294    0.176471         0.000000   \n",
       "3           0.842105          9.5    0.315789    0.105263         0.052632   \n",
       "4           1.000000          9.9    0.153846    0.153846         0.076923   \n",
       "\n",
       "   num_named_entities  contains_table  contains_list  redundancy_score  \\\n",
       "0                   1           False          False                 1   \n",
       "1                   0           False          False                 0   \n",
       "2                   0           False          False                 1   \n",
       "3                   0           False          False                 2   \n",
       "4                   1           False          False                 1   \n",
       "\n",
       "   compression_ratio  contains_numbers  contains_chain_of_thought  \\\n",
       "0           5.454545             False                      False   \n",
       "1           5.500000             False                      False   \n",
       "2           5.588235             False                      False   \n",
       "3           5.105263             False                      False   \n",
       "4           5.461538             False                      False   \n",
       "\n",
       "   contains_output_constraints  is_multi_turn  LLM  \n",
       "0                        False          False  GPT  \n",
       "1                        False          False  GPT  \n",
       "2                        False          False  GPT  \n",
       "3                        False          False  GPT  \n",
       "4                        False          False  GPT  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop PID column as it's just an identifier\n",
    "df = df.drop(columns=[\"PID\",\"data_type\"])\n",
    "# dropping data_type column as it is not needed for training. \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encode the target variable (LLM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>complexity</th>\n",
       "      <th>module</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>readability</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "      <th>adjective_ratio</th>\n",
       "      <th>num_named_entities</th>\n",
       "      <th>contains_table</th>\n",
       "      <th>contains_list</th>\n",
       "      <th>redundancy_score</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>contains_numbers</th>\n",
       "      <th>contains_chain_of_thought</th>\n",
       "      <th>contains_output_constraints</th>\n",
       "      <th>is_multi_turn</th>\n",
       "      <th>LLM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>Simple</td>\n",
       "      <td>document_extraction</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>Simple</td>\n",
       "      <td>document_extraction</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95</td>\n",
       "      <td>Simple</td>\n",
       "      <td>document_extraction</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5.588235</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97</td>\n",
       "      <td>Simple</td>\n",
       "      <td>document_extraction</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>5.105263</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>Simple</td>\n",
       "      <td>document_extraction</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5.461538</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_length complexity               module  num_tokens  num_sentences  \\\n",
       "0             60     Simple  document_extraction          11              1   \n",
       "1             55     Simple  document_extraction          10              1   \n",
       "2             95     Simple  document_extraction          17              1   \n",
       "3             97     Simple  document_extraction          19              1   \n",
       "4             71     Simple  document_extraction          13              1   \n",
       "\n",
       "   lexical_diversity  readability  noun_ratio  verb_ratio  adjective_ratio  \\\n",
       "0           0.909091          8.4    0.272727    0.181818         0.000000   \n",
       "1           1.000000          9.2    0.200000    0.200000         0.000000   \n",
       "2           0.941176         10.3    0.235294    0.176471         0.000000   \n",
       "3           0.842105          9.5    0.315789    0.105263         0.052632   \n",
       "4           1.000000          9.9    0.153846    0.153846         0.076923   \n",
       "\n",
       "   num_named_entities  contains_table  contains_list  redundancy_score  \\\n",
       "0                   1           False          False                 1   \n",
       "1                   0           False          False                 0   \n",
       "2                   0           False          False                 1   \n",
       "3                   0           False          False                 2   \n",
       "4                   1           False          False                 1   \n",
       "\n",
       "   compression_ratio  contains_numbers  contains_chain_of_thought  \\\n",
       "0           5.454545             False                      False   \n",
       "1           5.500000             False                      False   \n",
       "2           5.588235             False                      False   \n",
       "3           5.105263             False                      False   \n",
       "4           5.461538             False                      False   \n",
       "\n",
       "   contains_output_constraints  is_multi_turn  LLM  \n",
       "0                        False          False    1  \n",
       "1                        False          False    1  \n",
       "2                        False          False    1  \n",
       "3                        False          False    1  \n",
       "4                        False          False    1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df[\"LLM\"] = le.fit_transform(df[\"LLM\"])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate features and target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prompt_length', 'complexity', 'module', 'num_tokens', 'num_sentences',\n",
      "       'lexical_diversity', 'readability', 'noun_ratio', 'verb_ratio',\n",
      "       'adjective_ratio', 'num_named_entities', 'contains_table',\n",
      "       'contains_list', 'redundancy_score', 'compression_ratio',\n",
      "       'contains_numbers', 'contains_chain_of_thought',\n",
      "       'contains_output_constraints', 'is_multi_turn'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "y = df[\"LLM\"]\n",
    "X = df.drop(columns=[\"LLM\"])\n",
    "\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### one-hot encoding of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['complexity', 'module'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get categorical columns\n",
    "cat_columns = X.select_dtypes(['object']).columns\n",
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>readability</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "      <th>adjective_ratio</th>\n",
       "      <th>num_named_entities</th>\n",
       "      <th>contains_table</th>\n",
       "      <th>...</th>\n",
       "      <th>contains_output_constraints</th>\n",
       "      <th>is_multi_turn</th>\n",
       "      <th>complexity_Complex</th>\n",
       "      <th>complexity_Medium</th>\n",
       "      <th>complexity_Simple</th>\n",
       "      <th>module_document_extraction</th>\n",
       "      <th>module_interactive_qa_chatbot</th>\n",
       "      <th>module_multi_document_summarization</th>\n",
       "      <th>module_quantitative_analysis</th>\n",
       "      <th>module_report_generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>10.3</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_length  num_tokens  num_sentences  lexical_diversity  readability  \\\n",
       "0             60          11              1           0.909091          8.4   \n",
       "1             55          10              1           1.000000          9.2   \n",
       "2             95          17              1           0.941176         10.3   \n",
       "3             97          19              1           0.842105          9.5   \n",
       "4             71          13              1           1.000000          9.9   \n",
       "\n",
       "   noun_ratio  verb_ratio  adjective_ratio  num_named_entities  \\\n",
       "0    0.272727    0.181818         0.000000                   1   \n",
       "1    0.200000    0.200000         0.000000                   0   \n",
       "2    0.235294    0.176471         0.000000                   0   \n",
       "3    0.315789    0.105263         0.052632                   0   \n",
       "4    0.153846    0.153846         0.076923                   1   \n",
       "\n",
       "   contains_table  ...  contains_output_constraints  is_multi_turn  \\\n",
       "0           False  ...                        False          False   \n",
       "1           False  ...                        False          False   \n",
       "2           False  ...                        False          False   \n",
       "3           False  ...                        False          False   \n",
       "4           False  ...                        False          False   \n",
       "\n",
       "   complexity_Complex  complexity_Medium  complexity_Simple  \\\n",
       "0                 0.0                0.0                1.0   \n",
       "1                 0.0                0.0                1.0   \n",
       "2                 0.0                0.0                1.0   \n",
       "3                 0.0                0.0                1.0   \n",
       "4                 0.0                0.0                1.0   \n",
       "\n",
       "   module_document_extraction  module_interactive_qa_chatbot  \\\n",
       "0                         1.0                            0.0   \n",
       "1                         1.0                            0.0   \n",
       "2                         1.0                            0.0   \n",
       "3                         1.0                            0.0   \n",
       "4                         1.0                            0.0   \n",
       "\n",
       "   module_multi_document_summarization  module_quantitative_analysis  \\\n",
       "0                                  0.0                           0.0   \n",
       "1                                  0.0                           0.0   \n",
       "2                                  0.0                           0.0   \n",
       "3                                  0.0                           0.0   \n",
       "4                                  0.0                           0.0   \n",
       "\n",
       "   module_report_generation  \n",
       "0                       0.0  \n",
       "1                       0.0  \n",
       "2                       0.0  \n",
       "3                       0.0  \n",
       "4                       0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols = [\"complexity\", \"module\"]  # Exclude \"LLM\"\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "\n",
    "# Fit and transform only the selected categorical columns\n",
    "categorical_encoded = ohe.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Convert to DataFrame\n",
    "categorical_df = pd.DataFrame(categorical_encoded, columns=ohe.get_feature_names_out(categorical_cols))\n",
    "\n",
    "categorical_df.head()\n",
    "X = pd.concat([X, categorical_df], axis=1)\n",
    "X = X.drop(columns=categorical_cols)\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prompt_length', 'num_tokens', 'num_sentences', 'lexical_diversity',\n",
       "       'readability', 'noun_ratio', 'verb_ratio', 'adjective_ratio',\n",
       "       'num_named_entities', 'contains_table', 'contains_list',\n",
       "       'redundancy_score', 'compression_ratio', 'contains_numbers',\n",
       "       'contains_chain_of_thought', 'contains_output_constraints',\n",
       "       'is_multi_turn', 'complexity_Complex', 'complexity_Medium',\n",
       "       'complexity_Simple', 'module_document_extraction',\n",
       "       'module_interactive_qa_chatbot', 'module_multi_document_summarization',\n",
       "       'module_quantitative_analysis', 'module_report_generation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save subset for later use\n",
    "subset = X.head(4)\n",
    "subset.to_csv(\"files/X_train_subset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>readability</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "      <th>adjective_ratio</th>\n",
       "      <th>num_named_entities</th>\n",
       "      <th>contains_table</th>\n",
       "      <th>...</th>\n",
       "      <th>contains_output_constraints</th>\n",
       "      <th>is_multi_turn</th>\n",
       "      <th>complexity_Complex</th>\n",
       "      <th>complexity_Medium</th>\n",
       "      <th>complexity_Simple</th>\n",
       "      <th>module_document_extraction</th>\n",
       "      <th>module_interactive_qa_chatbot</th>\n",
       "      <th>module_multi_document_summarization</th>\n",
       "      <th>module_quantitative_analysis</th>\n",
       "      <th>module_report_generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>90</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>85</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>16.2</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>64</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt_length  num_tokens  num_sentences  lexical_diversity  readability  \\\n",
       "475             90          14              1           1.000000         13.9   \n",
       "499             85          14              1           1.000000         16.2   \n",
       "410             64          12              1           0.916667          7.6   \n",
       "273             52           9              1           1.000000          8.8   \n",
       "291             69          13              1           0.846154          9.2   \n",
       "\n",
       "     noun_ratio  verb_ratio  adjective_ratio  num_named_entities  \\\n",
       "475    0.285714    0.142857         0.214286                   0   \n",
       "499    0.214286    0.142857         0.142857                   1   \n",
       "410    0.250000    0.083333         0.083333                   0   \n",
       "273    0.444444    0.111111         0.111111                   1   \n",
       "291    0.230769    0.153846         0.000000                   0   \n",
       "\n",
       "     contains_table  ...  contains_output_constraints  is_multi_turn  \\\n",
       "475           False  ...                        False          False   \n",
       "499           False  ...                        False          False   \n",
       "410           False  ...                        False          False   \n",
       "273           False  ...                        False          False   \n",
       "291           False  ...                        False          False   \n",
       "\n",
       "     complexity_Complex  complexity_Medium  complexity_Simple  \\\n",
       "475                 0.0                0.0                1.0   \n",
       "499                 1.0                0.0                0.0   \n",
       "410                 0.0                0.0                1.0   \n",
       "273                 0.0                0.0                1.0   \n",
       "291                 0.0                1.0                0.0   \n",
       "\n",
       "     module_document_extraction  module_interactive_qa_chatbot  \\\n",
       "475                         0.0                            0.0   \n",
       "499                         0.0                            0.0   \n",
       "410                         0.0                            1.0   \n",
       "273                         0.0                            0.0   \n",
       "291                         0.0                            1.0   \n",
       "\n",
       "     module_multi_document_summarization  module_quantitative_analysis  \\\n",
       "475                                  1.0                           0.0   \n",
       "499                                  1.0                           0.0   \n",
       "410                                  0.0                           0.0   \n",
       "273                                  0.0                           0.0   \n",
       "291                                  0.0                           0.0   \n",
       "\n",
       "     module_report_generation  \n",
       "475                       0.0  \n",
       "499                       0.0  \n",
       "410                       0.0  \n",
       "273                       1.0  \n",
       "291                       0.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Train the classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],  # Number of boosting rounds\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],  # Step size shrinkage\n",
    "    \"max_depth\": [3, 6, 10],  # Maximum depth of a tree\n",
    "    \"min_child_weight\": [1, 3, 5],  # Minimum sum of instance weight needed in a child\n",
    "    \"subsample\": [0.8, 1.0],  # Fraction of samples used for training\n",
    "    \"colsample_bytree\": [0.8, 1.0],  # Fraction of features used per tree\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(objective=\"multi:softmax\", num_class=len(le.classes_), random_state=42, use_label_encoder=False)\n",
    "# Define the hyperparameter grid for XGBoost\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,  # Use all available processors\n",
    "    verbose=2,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = best_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGB model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Claude       0.67      0.50      0.57         8\n",
      "         GPT       0.82      0.82      0.82        11\n",
      "     Mistral       0.75      0.75      0.75        16\n",
      "        llma       0.78      0.88      0.82        16\n",
      "\n",
      "    accuracy                           0.76        51\n",
      "   macro avg       0.75      0.74      0.74        51\n",
      "weighted avg       0.76      0.76      0.76        51\n",
      "\n",
      "Accuracy: 0.76\n",
      "Precision: 0.76\n",
      "Recall: 0.76\n",
      "F1 Score: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert integer labels back to class names\n",
    "y_test_labels = le.inverse_transform(y_test)  # Convert encoded labels to original class names\n",
    "y_pred_labels = le.inverse_transform(y_pred)  # Convert predicted labels to original class names\n",
    "\n",
    "print(classification_report(y_test_labels, y_pred_labels, target_names=le.classes_))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "precision_score = precision_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Precision: {precision_score:.2f}\")\n",
    "recall_score = recall_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Recall: {recall_score:.2f}\")\n",
    "f1_score = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1 Score: {f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files/xgboost.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridsearchCV - Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.3s[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   4.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   4.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   4.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   5.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   5.1s[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   5.1s\n",
      "\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=50; total time=   5.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=200; total time=   5.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   5.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   5.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   5.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   5.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 3, 2, 3, 3, 2, 0, 3, 0, 3, 3, 0, 3, 2, 2, 2, 2, 3, 2, 2,\n",
       "       2, 3, 3, 0, 2, 3, 3, 3, 3, 2, 1, 3, 2, 2, 2, 2, 1, 1, 1, 3, 1, 1,\n",
       "       3, 0, 2, 3, 3, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],  # Number of trees in the forest\n",
    "    \"max_depth\": [10, 20, None],     # Maximum depth of the trees\n",
    "    \"min_samples_split\": [2, 5, 10], # Minimum number of samples required to split an internal node\n",
    "    \"min_samples_leaf\": [1, 2, 4]    # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,  # Use all available processors\n",
    "    verbose=2,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model from GridSearchCV\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the optimized model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model evaluation Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Claude       0.67      0.50      0.57         8\n",
      "         GPT       0.80      0.73      0.76        11\n",
      "     Mistral       0.75      0.75      0.75        16\n",
      "        llma       0.74      0.88      0.80        16\n",
      "\n",
      "    accuracy                           0.75        51\n",
      "   macro avg       0.74      0.71      0.72        51\n",
      "weighted avg       0.74      0.75      0.74        51\n",
      "\n",
      "Accuracy: 0.75\n",
      "Precision: 0.74\n",
      "Recall: 0.75\n",
      "F1 Score: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert integer labels back to class names\n",
    "y_test_labels = le.inverse_transform(y_test)  # Convert encoded labels to original class names\n",
    "y_pred_labels = le.inverse_transform(y_pred)  # Convert predicted labels to original class names\n",
    "\n",
    "print(classification_report(y_test_labels, y_pred_labels, target_names=le.classes_))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "precision_score = precision_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Precision: {precision_score:.2f}\")\n",
    "recall_score = recall_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Recall: {recall_score:.2f}\")\n",
    "f1_score = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1 Score: {f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files/v5_random_forest_onehot_encoder.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the MultiLabelBinarizer model\n",
    "joblib.dump(best_rf, \"files/best_random_forest.pkl\")\n",
    "\n",
    "joblib.dump(best_xgb, \"files/best_xgboost.pkl\") \n",
    "\n",
    "# Save the LabelEncoder\n",
    "joblib.dump(le, \"files/v5_random_forest_label_encoder.pkl\")\n",
    "\n",
    "# Save the StandardScaler\n",
    "joblib.dump(scaler, \"files/v5_random_forest_scaler.pkl\")\n",
    "\n",
    "# Save the OneHotEncoder\n",
    "joblib.dump(ohe, \"files/v5_random_forest_onehot_encoder.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textstat\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_prompt_features(prompt):\n",
    "    doc = nlp(prompt)\n",
    "    \n",
    "    #  Linguistic Features\n",
    "    num_tokens = len(doc)\n",
    "    num_sentences = len(list(doc.sents))\n",
    "    unique_words = len(set([token.text.lower() for token in doc]))\n",
    "    lexical_diversity = unique_words / num_tokens if num_tokens > 0 else 0\n",
    "    readability = textstat.flesch_kincaid_grade(prompt)\n",
    "    \n",
    "    # Part-of-Speech (POS) Distribution\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    noun_ratio = pos_counts.get(\"NOUN\", 0) / num_tokens\n",
    "    verb_ratio = pos_counts.get(\"VERB\", 0) / num_tokens\n",
    "    adjective_ratio = pos_counts.get(\"ADJ\", 0) / num_tokens\n",
    "\n",
    "    #  Structural Features\n",
    "    num_named_entities = len(doc.ents)\n",
    "    contains_table = bool(re.search(r\"(\\||\\+\\-+)|Table \\d+\", prompt))  # Detect tables\n",
    "    contains_list = bool(re.search(r\"(\\d+\\.)|(- )|(\\* )\", prompt))  # Detect bullet points or numbered lists\n",
    "\n",
    "    # Content-Specific Features\n",
    "    keyword_density = {word: prompt.lower().count(word) for word in [\"summary\", \"analyze\", \"data\", \"report\"]}\n",
    "    redundancy_score = len(re.findall(r\"(\\b\\w+\\b).*\\1\", prompt))  # Count repeated words\n",
    "    compression_ratio = len(prompt) / num_tokens if num_tokens > 0 else 0\n",
    "\n",
    "    #  Task-Specific Complexity Features\n",
    "    contains_numbers = bool(re.search(r\"\\d+\", prompt))  # Presence of numerical data\n",
    "    contains_chain_of_thought = bool(re.search(r\"step-by-step|explain your reasoning\", prompt.lower()))\n",
    "    contains_output_constraints = bool(re.search(r\"limit to|output in|return a json\", prompt.lower()))\n",
    "    is_multi_turn = \"previous response\" in prompt.lower() or \"as mentioned before\" in prompt.lower()\n",
    "\n",
    "    return {\n",
    "        \"num_tokens\": num_tokens,\n",
    "        \"num_sentences\": num_sentences,\n",
    "        \"lexical_diversity\": lexical_diversity,\n",
    "        \"readability\": readability,\n",
    "        \"noun_ratio\": noun_ratio,\n",
    "        \"verb_ratio\": verb_ratio,\n",
    "        \"adjective_ratio\": adjective_ratio,\n",
    "        \"num_named_entities\": num_named_entities,\n",
    "        \"contains_table\": contains_table,\n",
    "        \"contains_list\": contains_list,\n",
    "        \"redundancy_score\": redundancy_score,\n",
    "        \"compression_ratio\": compression_ratio,\n",
    "        \"contains_numbers\": contains_numbers,\n",
    "        \"contains_chain_of_thought\": contains_chain_of_thought,\n",
    "        \"contains_output_constraints\": contains_output_constraints,\n",
    "        \"is_multi_turn\": is_multi_turn\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import joblib\n",
    "# load the model and use it to make new predictions\n",
    "model = joblib.load(\"files/best_xgboost.pkl\")\n",
    "scaler = joblib.load(\"files/v5_random_forest_scaler.pkl\")\n",
    "ohe = joblib.load(\"files/v5_random_forest_onehot_encoder.pkl\")\n",
    "le = joblib.load(\"files/v5_random_forest_label_encoder.pkl\")\n",
    "\n",
    "\n",
    "def classify_complexity(prompt, num_tokens=25, readability=70, num_sentences=1, lexical_diversity=2, num_named_entities=3):\n",
    "    if num_tokens <= 10 and readability >= 60 and num_sentences == 1:\n",
    "        return \"Simple\"\n",
    "    elif 10 < num_tokens <= 20 or (40 <= readability < 60) or (num_named_entities <= 4):\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Complex\"\n",
    "\n",
    "\n",
    "print()  # Output: Medium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a prompt for summarizing a text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_tokens': 9, 'num_sentences': 1, 'lexical_diversity': 0.8888888888888888, 'readability': 2.9, 'noun_ratio': 0.2222222222222222, 'verb_ratio': 0.0, 'adjective_ratio': 0.1111111111111111, 'num_named_entities': 0, 'contains_table': False, 'contains_list': False, 'redundancy_score': 1, 'compression_ratio': 4.444444444444445, 'contains_numbers': False, 'contains_chain_of_thought': False, 'contains_output_constraints': False, 'is_multi_turn': False, 'complexity': 'Medium', 'module': 'document_extraction', 'prompt_length': 40}\n",
      "Predicted class: GPT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Convert input into DataFrame\n",
    "# new_df = pd.DataFrame([new_input])\n",
    "# Example Prompt\n",
    "prompt = \"\"\"\n",
    "You are an AI assistant. Summarize the research paper and provide key insights. \n",
    "Ensure the response is formatted as JSON and limited to 200 words. \n",
    "Step-by-step analysis is preferred. Example: \"In this study, the authors examined...\"\n",
    "\"\"\"\n",
    "prompt = \"What was the main finding of the report?\"\n",
    "# Extract Features\n",
    "features = {}\n",
    "X_train_subset = pd.read_csv(\"files/X_train_subset.csv\")\n",
    "\n",
    "features = extract_prompt_features(prompt)\n",
    "complexity = classify_complexity(prompt)\n",
    "features.update({\"complexity\": complexity, \"module\": \"document_extraction\",\"prompt_length\": len(prompt)})\n",
    "print(features)\n",
    "\n",
    "df = pd.DataFrame([features],index=[0])\n",
    "\n",
    "# Transform the input using the saved encoders\n",
    "\n",
    "\n",
    "categorical_cols = [\"complexity\", \"module\"]\n",
    "categorical_encoded = ohe.transform(df[categorical_cols])\n",
    "categorical_df = pd.DataFrame(categorical_encoded, columns=ohe.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "df = pd.concat([df, categorical_df], axis=1)\n",
    "new_df = df.drop(columns=categorical_cols)\n",
    "\n",
    "new_df.head()\n",
    "new_df = new_df[X_train_subset.columns]\n",
    "new_df = scaler.transform(new_df)\n",
    "\n",
    "# # # Make a prediction\n",
    "prediction = model.predict(new_df)\n",
    "predicted_class = le.inverse_transform(prediction)[0]\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "\n",
    "# Index(['prompt_length', 'complexity', 'module', 'num_tokens', 'num_sentences',\n",
    "#        'lexical_diversity', 'readability', 'noun_ratio', 'verb_ratio',\n",
    "#        'adjective_ratio', 'num_named_entities', 'contains_table',\n",
    "#        'contains_list', 'redundancy_score', 'compression_ratio',\n",
    "#        'contains_numbers', 'contains_chain_of_thought',\n",
    "#        'contains_output_constraints', 'is_multi_turn', 'LLM'],\n",
    "#       dtype='object')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23584124 0.33676404 0.22134596 0.20604873]]\n",
      "Predicted Class: 1\n",
      "Confidence Score: 33.68%\n"
     ]
    }
   ],
   "source": [
    "prob_scores = model.predict_proba(new_df)\n",
    "print(prob_scores)\n",
    "# Get predicted class and confidence score\n",
    "predicted_class = model.predict(new_df)[0]\n",
    "confidence_score = np.max(prob_scores) * 100  # Convert to percentage\n",
    "\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Confidence Score: {confidence_score:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mispix_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
