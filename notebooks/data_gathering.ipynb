{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PIP installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !pip install textstat\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install langchain_mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "        with open(image_path, \"rb\") as img_file:\n",
    "                return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "def query_llm_rest_pix(mlist,image=None):\n",
    "        \n",
    "        url = 'http://172.19.104.12:9104/v1/chat/completions'\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Authorization': 'Bearer token'\n",
    "        }\n",
    "\n",
    "\n",
    "        data = {\n",
    "            \"model\": \"mistralai/Pixtral-12B-2409\"\n",
    "        }\n",
    "        data.update({\"messages\": mlist})\n",
    "\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "        if response.status_code == 400:\n",
    "            return \"I'm unsure about the answer. Please provide more context or ask a different question.\"\n",
    "        else:\n",
    "            return response.json()[\"choices\"][0]['message']['content']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sythetic dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Creation Approach\n",
    "\n",
    "    There are lot of datasets available on the internet for varies llm evaluation tasks. \n",
    "\n",
    "    Dataset benchmarks like \n",
    "```\n",
    "    arc-challenge\n",
    "    mbpp\n",
    "    winogrande\n",
    "    mtbench\n",
    "    grade-school-math\n",
    "    hellaswag\n",
    "    mmlu\n",
    "```\n",
    "    However, categorizing, downloading and preprocessing those datasets was taking more time for me than anticipated initially. Because of this reason, I will be taking a sythetic dataset creation approach to generate the data.\n",
    "\n",
    "##### I will be using the LLMs to generate the data for the modules defined in the assignment.\n",
    "```\n",
    "    Document Extraction\n",
    "    Quantitative Analysis\n",
    "    Report Generation\n",
    "    Interactive QA Chatbot\n",
    "    Multi Document Summarization\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define module definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = {\n",
    "    \"t1\":\"Document Content Extraction: Reading documents in different formats and extracting information from it, which could be in tables, graphs, etc. Make sure all quantitative data is extracted correctly with the right context.\",\n",
    "    \"t2\": \"Quantitative Analysis: To streamline and automate the process of performing advanced calculations based on given parameters. If required, the LLM needs to choose the correct parameters like assumptions, margin of error, etc.\",\n",
    "    \"t3\": \"Report Generation: Creating a report following a pre-set format, length, and style, including text, charts, and illustrations. This can be calibrated based on requirements and data sources.\",\n",
    "    \"t4\":\"Interactive QA Chatbot: A question-and-answer chatbot to go with the final report so that if there are any queries, they can be answered based on all the sources used for creating the report.\",\n",
    "    \"t5\":\"Multi-document Summarization: Summarizing data from multiple documents, giving importance to the most recent information or specified context.\"\n",
    "}\n",
    "\n",
    "maping = {\n",
    "\"t1\":\"document_extraction\",\n",
    "\"t2\":\"quantitative_analysis\",\n",
    "\"t3\":\"report_generation\",\n",
    "\"t4\":\"interactive_qa_chatbot\",\n",
    "\"t5\":\"multi_document_summarization\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prompt Iteration \n",
    "\n",
    "I refined and enhanced the prompt to optimize prompt generation. Below is the final version that was used. I'm using pixtral local model here. Using bigger model will certainly yield better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "    You are a prompt generation expert who can generate prompts for variety of tasks. You have been given the following description of task. \n",
    "    The prompt you generate should be of varying length and complexity. Give the response stricty in python dictionary format.\n",
    "    {{\"prompt\":\"\",\"complexity\":\"\",\"data_type\":\"\"}}\n",
    "\n",
    "    data_type can include following types[\"Text\", \"Numerical Data\", \"Tables\", \"Graphs & Charts\", \"Equations & Formulas\", \"Dates & Timestamps\"]\n",
    "    complexity can include following types[\"Simple\", \"Medium\", \"Complex\"]\n",
    "    \n",
    "    Reponse only in text. Dont include markdown.\n",
    "    Generate 50 prompts for each of the following tasks:\n",
    "        Document Content Extraction: Reading documents in different formats and extracting information from it, which could be in tables, graphs, etc. Make sure all quantitative data is extracted correctly with the right context.\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "num_prompts = 50\n",
    "\n",
    "# init llm\n",
    "llm = get_llm()\n",
    "data = []\n",
    "\n",
    "# loop through modules\n",
    "for name, module in modules.items():\n",
    "    prompt = f\"\"\"\n",
    "    You are a prompt generation expert who can generate prompts for variety of tasks. You have been given the following description of task. \n",
    "    The prompt you generate should be of varying length and complexity. Give the response stricty in python dictionary format.\n",
    "    {{\"prompt\":\"\",\"complexity\":\"\",\"data_type\":\"\"}}\n",
    "\n",
    "    data_type can include following types[\"Text\", \"Numerical Data\", \"Tables\", \"Graphs & Charts\", \"Equations & Formulas\", \"Dates & Timestamps\"]\n",
    "    complexity can include following types[\"Simple\", \"Medium\", \"Complex\"]\n",
    "    \n",
    "    Reponse only in text. Dont include markdown.\n",
    "    Generate {num_prompts} prompts for each of the following tasks:\n",
    "        {module}\n",
    "\n",
    "    \"\"\"\n",
    "    mlist =[ {       \n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"You are a prompt generation expert who can generate prompts for variety of tasks.\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt}\n",
    "                    ]\n",
    "        }\n",
    "    ]\n",
    "    res = llm.invoke(message)\n",
    "    print(res.content)\n",
    "    data.append({\"module\":name,\"prompts\":res})\n",
    "    print(data)\n",
    "    \n",
    "with open(\"v1.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Explanations for LLM Prompt Classification\n",
    "\n",
    "- **`num_tokens`** – Helps determine prompt length, affecting processing time and model performance.  \n",
    "- **`num_sentences`** – Indicates prompt complexity and structure.  \n",
    "- **`lexical_diversity`** – Measures vocabulary richness, impacting generalization.  \n",
    "- **`readability`** – Assesses how easy or difficult the prompt is to understand.  \n",
    "- **`noun_ratio`** – Identifies the amount of entity-related information.  \n",
    "- **`verb_ratio`** – Shows action-oriented nature of the prompt.  \n",
    "- **`adjective_ratio`** – Captures descriptive complexity in the prompt.  \n",
    "- **`num_named_entities`** – Detects presence of key entities, aiding context understanding.  \n",
    "- **`contains_table`** – Flags structured data presence, affecting LLM parsing needs.  \n",
    "- **`contains_list`** – Identifies enumerated elements that may require special processing.  \n",
    "- **`keyword_density`** – Measures topic emphasis and focus within the prompt.  \n",
    "- **`redundancy_score`** – Helps detect excessive repetition, which may confuse LLMs.  \n",
    "- **`compression_ratio`** – Evaluates information density, useful for summarization tasks.  \n",
    "- **`contains_numbers`** – Determines presence of quantitative data, influencing numerical reasoning.  \n",
    "- **`contains_chain_of_thought`** – Checks for step-by-step reasoning, important for logical tasks.  \n",
    "- **`contains_output_constraints`** – Identifies explicit formatting constraints in expected responses.  \n",
    "- **`is_multi_turn`** – Detects conversational prompts requiring memory or context retention.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textstat\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_prompt_features(prompt):\n",
    "    doc = nlp(prompt)\n",
    "    \n",
    "    #  Linguistic Features\n",
    "    num_tokens = len(doc)\n",
    "    num_sentences = len(list(doc.sents))\n",
    "    unique_words = len(set([token.text.lower() for token in doc]))\n",
    "    lexical_diversity = unique_words / num_tokens if num_tokens > 0 else 0\n",
    "    readability = textstat.flesch_kincaid_grade(prompt)\n",
    "    \n",
    "    # Part-of-Speech (POS) Distribution\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    noun_ratio = pos_counts.get(\"NOUN\", 0) / num_tokens\n",
    "    verb_ratio = pos_counts.get(\"VERB\", 0) / num_tokens\n",
    "    adjective_ratio = pos_counts.get(\"ADJ\", 0) / num_tokens\n",
    "\n",
    "    #  Structural Features\n",
    "    num_named_entities = len(doc.ents)\n",
    "    contains_table = bool(re.search(r\"(\\||\\+\\-+)|Table \\d+\", prompt))  # Detect tables\n",
    "    contains_list = bool(re.search(r\"(\\d+\\.)|(- )|(\\* )\", prompt))  # Detect bullet points or numbered lists\n",
    "\n",
    "    # Content-Specific Features\n",
    "    keyword_density = {word: prompt.lower().count(word) for word in [\"summary\", \"analyze\", \"data\", \"report\"]}\n",
    "    redundancy_score = len(re.findall(r\"(\\b\\w+\\b).*\\1\", prompt))  # Count repeated words\n",
    "    compression_ratio = len(prompt) / num_tokens if num_tokens > 0 else 0\n",
    "\n",
    "    #  Task-Specific Complexity Features\n",
    "    contains_numbers = bool(re.search(r\"\\d+\", prompt))  # Presence of numerical data\n",
    "    contains_chain_of_thought = bool(re.search(r\"step-by-step|explain your reasoning\", prompt.lower()))\n",
    "    contains_output_constraints = bool(re.search(r\"limit to|output in|return a json\", prompt.lower()))\n",
    "    is_multi_turn = \"previous response\" in prompt.lower() or \"as mentioned before\" in prompt.lower()\n",
    "\n",
    "    return {\n",
    "        \"num_tokens\": num_tokens,\n",
    "        \"num_sentences\": num_sentences,\n",
    "        \"lexical_diversity\": lexical_diversity,\n",
    "        \"readability\": readability,\n",
    "        \"noun_ratio\": noun_ratio,\n",
    "        \"verb_ratio\": verb_ratio,\n",
    "        \"adjective_ratio\": adjective_ratio,\n",
    "        \"num_named_entities\": num_named_entities,\n",
    "        \"contains_table\": contains_table,\n",
    "        \"contains_list\": contains_list,\n",
    "        \"redundancy_score\": redundancy_score,\n",
    "        \"compression_ratio\": compression_ratio,\n",
    "        \"contains_numbers\": contains_numbers,\n",
    "        \"contains_chain_of_thought\": contains_chain_of_thought,\n",
    "        \"contains_output_constraints\": contains_output_constraints,\n",
    "        \"is_multi_turn\": is_multi_turn\n",
    "    }\n",
    "\n",
    "# Example Prompt\n",
    "prompt = \"\"\"\n",
    "You are an AI assistant. Summarize the research paper and provide key insights. \n",
    "Ensure the response is formatted as JSON and limited to 200 words. \n",
    "Step-by-step analysis is preferred. Example: \"In this study, the authors examined...\"\n",
    "\"\"\"\n",
    "\n",
    "# Extract Features\n",
    "features = extract_prompt_features(prompt)\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vikasnr/codebase/crsl/tuesday/project_ulti/notebooks\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_llm' from 'llm' (/home/vikasnr/codebase/crsl/tuesday/project_ulti/notebooks/../llm.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/vikasnr/codebase/crsl/tuesday/project_ulti\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_llm\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_llm' from 'llm' (/home/vikasnr/codebase/crsl/tuesday/project_ulti/notebooks/../llm.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "print(os.getcwd())\n",
    "sys.path.append(os.path.join('/home/vikasnr/codebase/crsl/tuesday/project_ulti'))\n",
    "\n",
    "from llm import get_llm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mispix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
