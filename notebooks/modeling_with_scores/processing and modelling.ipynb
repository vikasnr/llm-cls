{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLM\n",
       "GPT        186\n",
       "Claude     170\n",
       "Mistral    144\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "df = pd.read_csv('/home/vikasnr/codebase/crsl/tuesday/llm-cls/method_1/llm_selection_data_500.csv')\n",
    "df.drop(columns=[\"domain\"],inplace=True)\n",
    "df.LLM.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_type</th>\n",
       "      <th>input_token_length</th>\n",
       "      <th>output_token_length</th>\n",
       "      <th>multimodal</th>\n",
       "      <th>structured_input</th>\n",
       "      <th>numerical_reasoning</th>\n",
       "      <th>multi_turn_conversation</th>\n",
       "      <th>grounding_required</th>\n",
       "      <th>creativity_level</th>\n",
       "      <th>response_factuality</th>\n",
       "      <th>...</th>\n",
       "      <th>context_window</th>\n",
       "      <th>memory_usage_GB</th>\n",
       "      <th>cost_per_1K_tokens</th>\n",
       "      <th>on_prem_support</th>\n",
       "      <th>cloud_provider</th>\n",
       "      <th>gpu_availability</th>\n",
       "      <th>fine_tuning_available</th>\n",
       "      <th>compliance_requirements</th>\n",
       "      <th>bias_score</th>\n",
       "      <th>LLM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Code Generation</td>\n",
       "      <td>46</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4096</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.01656</td>\n",
       "      <td>0</td>\n",
       "      <td>Azure</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HIPAA</td>\n",
       "      <td>0.595</td>\n",
       "      <td>Claude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Code Generation</td>\n",
       "      <td>904</td>\n",
       "      <td>1094</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>16384</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0.00895</td>\n",
       "      <td>1</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>GDPR</td>\n",
       "      <td>0.933</td>\n",
       "      <td>Mistral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Classification</td>\n",
       "      <td>560</td>\n",
       "      <td>344</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>16384</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.01584</td>\n",
       "      <td>0</td>\n",
       "      <td>AWS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>HIPAA</td>\n",
       "      <td>0.446</td>\n",
       "      <td>GPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Classification</td>\n",
       "      <td>349</td>\n",
       "      <td>568</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>16384</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.00659</td>\n",
       "      <td>1</td>\n",
       "      <td>GCP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GDPR</td>\n",
       "      <td>0.954</td>\n",
       "      <td>Claude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Summarization</td>\n",
       "      <td>261</td>\n",
       "      <td>1757</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>16384</td>\n",
       "      <td>34.3</td>\n",
       "      <td>0.01144</td>\n",
       "      <td>1</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.558</td>\n",
       "      <td>Mistral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         task_type  input_token_length  output_token_length  multimodal  \\\n",
       "0  Code Generation                  46                  202           0   \n",
       "1  Code Generation                 904                 1094           0   \n",
       "2   Classification                 560                  344           1   \n",
       "3   Classification                 349                  568           0   \n",
       "4    Summarization                 261                 1757           1   \n",
       "\n",
       "   structured_input  numerical_reasoning  multi_turn_conversation  \\\n",
       "0                 1                    0                        0   \n",
       "1                 0                    0                        1   \n",
       "2                 0                    1                        1   \n",
       "3                 0                    0                        1   \n",
       "4                 1                    1                        1   \n",
       "\n",
       "   grounding_required  creativity_level  response_factuality  ...  \\\n",
       "0                   1                 5                    2  ...   \n",
       "1                   0                 3                    1  ...   \n",
       "2                   1                 1                    5  ...   \n",
       "3                   1                 1                    3  ...   \n",
       "4                   1                 2                    1  ...   \n",
       "\n",
       "   context_window  memory_usage_GB  cost_per_1K_tokens  on_prem_support  \\\n",
       "0            4096             33.6             0.01656                0   \n",
       "1           16384             34.5             0.00895                1   \n",
       "2           16384             23.5             0.01584                0   \n",
       "3           16384             26.6             0.00659                1   \n",
       "4           16384             34.3             0.01144                1   \n",
       "\n",
       "   cloud_provider  gpu_availability  fine_tuning_available  \\\n",
       "0           Azure                 1                      1   \n",
       "1          OpenAI                 0                      1   \n",
       "2             AWS                 1                      1   \n",
       "3             GCP                 0                      0   \n",
       "4          OpenAI                 1                      1   \n",
       "\n",
       "   compliance_requirements  bias_score      LLM  \n",
       "0                    HIPAA       0.595   Claude  \n",
       "1                     GDPR       0.933  Mistral  \n",
       "2                    HIPAA       0.446      GPT  \n",
       "3                     GDPR       0.954   Claude  \n",
       "4                      NaN       0.558  Mistral  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /home/vikasnr/miniconda3/envs/mispix_clone/lib/python3.10/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /home/vikasnr/miniconda3/envs/mispix_clone/lib/python3.10/site-packages (from imbalanced-learn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /home/vikasnr/miniconda3/envs/mispix_clone/lib/python3.10/site-packages (from imbalanced-learn) (1.5.2)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
      "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /home/vikasnr/miniconda3/envs/mispix_clone/lib/python3.10/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /home/vikasnr/miniconda3/envs/mispix_clone/lib/python3.10/site-packages (from imbalanced-learn) (3.5.0)\n",
      "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Downloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44642857142857145\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Claude       0.24      0.30      0.26        30\n",
      "         GPT       0.48      0.42      0.45        38\n",
      "     Mistral       0.61      0.57      0.59        44\n",
      "\n",
      "    accuracy                           0.45       112\n",
      "   macro avg       0.44      0.43      0.43       112\n",
      "weighted avg       0.47      0.45      0.45       112\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikasnr/miniconda3/envs/mispix_clone/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [11:27:21] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "# categorical_cols = [\"task_type\", \"domain\", \"cloud_provider\", \"compliance_requirements\"]\n",
    "categorical_cols = [\"task_type\", \"cloud_provider\", \"compliance_requirements\"]\n",
    "\n",
    "numerical_cols = [col for col in df.columns if col not in categorical_cols + [\"LLM\"]]\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {col: LabelEncoder() for col in categorical_cols}\n",
    "for col in categorical_cols:\n",
    "    df[col] = label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "# Encode target variable\n",
    "target_encoder = LabelEncoder()\n",
    "df[\"LLM\"] = target_encoder.fit_transform(df[\"LLM\"])\n",
    "\n",
    "# Separate target variable (LLM) and features\n",
    "X = df.drop(columns=[\"LLM\"])\n",
    "y = df[\"LLM\"]\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an XGBoost classifier\n",
    "clf = XGBClassifier(n_estimators=50, max_depth=3, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=target_encoder.classes_)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\\n\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the nature of the data and the goal of predicting the 'LLM' (Large Language Model), some features might be considered less impactful or potentially redundant. Here's a breakdown of features that could be considered for ignoring or further analysis:\n",
    "\n",
    "# Potentially Ignorable or Less Impactful Features:\n",
    "\n",
    "# input_token_length and output_token_length:\n",
    "# While these features provide information about the size of the input and output, their direct correlation with the specific LLM might be complex. LLM selection is often driven by capabilities, cost, and compliance more than raw token counts. However, they could be useful for performance analysis, not direct LLM prediction.\n",
    "# latency_ms and token_throughput:\n",
    "# These are performance metrics that can vary significantly based on hardware, network conditions, and specific usage patterns. They might be more reflective of the deployment environment than the inherent characteristics of the LLM. Again, usefull for performance analysis.\n",
    "# memory_usage_GB:\n",
    "# Similar to latency and throughput, memory usage is highly dependent on the deployment environment and the specific task.\n",
    "# BLEU_score, ROUGE_score, accuracy_score, faithfulness_score, hallucination_rate, bias_score:\n",
    "# These are evaluation metrics that reflect the performance of the LLM on specific tasks. They are results of using the LLM, not intrinsic properties that determine which LLM is chosen. Because of this, they are usefull to evaluate the model, but not to predict which model was used.\n",
    "# context_window:\n",
    "# While context window size is a characteristic of an LLM, it might be highly correlated with other features (like memory_usage_GB or gpu_availability). If other features already capture the essence of computational requirements, this might be redundant.\n",
    "# Reasons for Potential Redundancy:\n",
    "\n",
    "# Deployment Dependence: Many performance-related metrics (latency, throughput, memory) are influenced by factors outside the LLM itself. Â  \n",
    "# Resultant Metrics: Evaluation scores are the outcome of using the LLM, not its defining features.\n",
    "# Correlation: Some features might be highly correlated, providing redundant information. Â  \n",
    "# Important Considerations:\n",
    "\n",
    "# Domain Expertise: The significance of features can vary depending on the specific domain. If the analysis is focused on performance optimization, then latency and throughput become crucial.\n",
    "# Feature Importance Analysis: Techniques like feature importance from tree-based models (e.g., Random Forest) or permutation importance can help quantify the contribution of each feature to the model's predictions. Â  \n",
    "# Recommendation:\n",
    "\n",
    "# Start by training a model with all features.\n",
    "# Then, perform feature importance analysis to identify the most influential features.\n",
    "# Experiment with removing the potentially redundant or less impactful features and observe the model's performance.\n",
    "# By carefully considering these factors, you can refine your feature set and build a more robust and efficient model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__max_depth': None, 'classifier__min_samples_leaf': 4, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 100}\n",
      "Accuracy: 0.33\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Claude       0.33      0.21      0.26        38\n",
      "         GPT       0.34      0.54      0.42        39\n",
      "     Mistral       0.29      0.17      0.22        23\n",
      "\n",
      "    accuracy                           0.33       100\n",
      "   macro avg       0.32      0.31      0.30       100\n",
      "weighted avg       0.32      0.33      0.31       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import io\n",
    "df = pd.read_csv('/home/vikasnr/codebase/crsl/tuesday/llm-cls/method_1/llm_selection_data_500.csv')\n",
    "df.drop(columns=[\"domain\"],inplace=True)\n",
    "df.head()\n",
    "df.LLM.value_counts()\n",
    "data = df\n",
    "# import io\n",
    "# data = pd.read_csv(io.StringIO(data))\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('LLM', axis=1)\n",
    "y = data['LLM']\n",
    "\n",
    "# Drop potentially less impactful features\n",
    "X = X.drop([\n",
    "    'input_token_length', 'output_token_length', 'latency_ms', 'token_throughput',\n",
    "    'memory_usage_GB', 'BLEU_score', 'ROUGE_score', 'accuracy_score',\n",
    "    'faithfulness_score', 'hallucination_rate', 'bias_score', 'context_window'\n",
    "], axis=1, errors='ignore')\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create the model pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42)) # Using RandomForestClassifier\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.31\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Claude       0.36      0.34      0.35        38\n",
      "         GPT       0.35      0.38      0.37        39\n",
      "     Mistral       0.14      0.13      0.14        23\n",
      "\n",
      "    accuracy                           0.31       100\n",
      "   macro avg       0.28      0.29      0.28       100\n",
      "weighted avg       0.31      0.31      0.31       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import io\n",
    "df = pd.read_csv('/home/vikasnr/codebase/crsl/tuesday/llm-cls/method_1/llm_selection_data_500.csv')\n",
    "df.drop(columns=[\"domain\"],inplace=True)\n",
    "df.head()\n",
    "df.LLM.value_counts()\n",
    "\n",
    "data = df\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('LLM', axis=1)\n",
    "y = data['LLM']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create the model pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mispix_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
